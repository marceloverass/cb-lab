{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0cf0fd5-d308-4775-9a14-d787df928fdb",
   "metadata": {},
   "source": [
    "# Desafio 2: Arquitetura de Data Lake e Pipeline de Ingestão \n",
    "\n",
    "**Autor:** Marcelo Veras<br>\n",
    "**Data:** 29 de Julho de 2025\n",
    "\n",
    "## 1. Introdução e Arquitetura\n",
    "\n",
    "Este notebook documenta a solução para o Desafio 2, que foca na criação de um pipeline de ingestão de dados para popular um Data Lake. O objetivo é consumir dados de 5 endpoints de API de uma rede de restaurantes e armazená-los de forma bruta, preparando o terreno para futuras transformações e análises.\n",
    "\n",
    "A solução foi projetada utilizando a **Arquitetura Medallion**, um padrão moderno que organiza os dados em três camadas lógicas de qualidade, garantindo robustez, rastreabilidade e flexibilidade ao fluxo de dados.\n",
    "\n",
    "## 1.1. Arquitetura de Dados: A Arquitetura Medallion\n",
    "\n",
    "Para garantir a qualidade, rastreabilidade e flexibilidade dos dados, foi implementada a **Arquitetura Medallion**, que organiza o Data Lake em três camadas distintas.\n",
    "\n",
    "* Os dados são processados através das camadas **Bronze, Silver e Gold**.\n",
    "    * **Bronze:** Armazena os dados brutos das APIs em formato JSON, sem nenhuma modificação.\n",
    "    * **Silver:** Contém os dados limpos, padronizados, com tipos de dados corrigidos e enriquecidos, salvos em formato Parquet, que é otimizado para análise.\n",
    "    * **Gold:** É o nosso Data Warehouse final no SQL Server, com um modelo de dados relacional e normalizado, pronto para o consumo por ferramentas de BI.\n",
    "* **Justificativa:**\n",
    "    * **Qualidade Progressiva:** Essa abordagem permite isolar as etapas de processamento. A camada Bronze garante que nunca percamos os dados originais. A camada Silver fornece uma base de dados limpa e confiável para todas as futuras análises, e a camada Gold serve dados agregados e específicos para as necessidades de negócio.\n",
    "    * **Reprocessamento e Resiliência:** Se um bug for encontrado na lógica de transformação, podemos corrigir o código e reprocessar os dados a partir da camada Bronze ou Silver para a Gold, sem a necessidade de re-ingerir os dados das APIs de origem.\n",
    "    * **Flexibilidade:** Diferentes equipes podem consumir os dados da camada que melhor lhes convém. Analistas de BI usariam a camada Gold, enquanto Cientistas de Dados poderiam preferir a camada Silver para exploração.\n",
    "\n",
    "### 2. Por que armazenar as respostas das APIs?\n",
    "\n",
    "Armazenar as respostas brutas e imutáveis das APIs em um Data Lake (a camada **Bronze**) é uma prática fundamental na engenharia de dados moderna. As principais razões são:\n",
    "\n",
    "* **Fonte da Verdade e Rastreabilidade:** O Data Lake se torna a \"fonte única da verdade\", contendo os dados exatamente como foram recebidos da origem. Isso cria um registro histórico completo e imutável, essencial para auditorias e para garantir a rastreabilidade de cada informação.\n",
    "* **Capacidade de Reprocessamento:** Regras de negócio e lógicas de transformação mudam com o tempo. Ao manter uma cópia fiel dos dados brutos, garantimos a capacidade de reprocessar o histórico com novas regras, corrigir bugs no pipeline ou criar novos modelos de dados (Data Marts) sem a necessidade de consultar as APIs de origem novamente, o que pode ser lento, caro ou até impossível.\n",
    "* **Resiliência a Mudanças de Schema (Evolução de Schema):** Os sistemas de origem podem alterar o formato dos dados. Ao salvar o dado bruto \"como está\", garantimos que nenhuma informação seja perdida. O pipeline de ingestão continua funcionando, e a falha (se houver) ocorrerá apenas na etapa de transformação, que poderá então ser ajustada para lidar com o novo schema.\n",
    "* **Flexibilidade para Futuros Casos de Uso:** O modelo relacional que criado no Desafio 1 (camada Gold) é otimizado para um propósito analítico específico. Os dados brutos, no entanto, podem ser extremamente valiosos para outras equipes, como Cientistas de Dados, que podem querer explorá-los para descobrir novos padrões ou treinar modelos de Machine Learning.\n",
    "\n",
    "### 2.1. Como os dados seriam armazenados? (Estrutura do Data Lake)\n",
    "\n",
    "A abordagem escolhida para estruturar o Data Lake foi a de um sistema de pastas hierárquico e **particionado**, o que permite buscas e processamento de dados em larga escala de forma extremamente eficiente.\n",
    "\n",
    "A estrutura de pastas na camada **Bronze** é a seguinte:\n",
    "\n",
    "```\n",
    "data-lake/\n",
    "└── bronze/\n",
    "    ├── [nome_do_endpoint]/\n",
    "    │   └── year=[AAAA]/\n",
    "    │       └── month=[MM]/\n",
    "    │           └── day=[DD]/\n",
    "    │               └── storeId=[id_da_loja]_data.json\n",
    "```\n",
    "\n",
    "**Justificativa desta estrutura:**\n",
    "\n",
    "* **`bronze/`**: Identifica a camada de dados brutos e imutáveis.\n",
    "* **`[nome_do_endpoint]/`**: A primeira partição é pela **fonte de dados** (ex: `/res/getGuestChecks/`). Isso isola os dados de cada API e permite o processamento independente.\n",
    "* **`year=[AAAA]`, `month=[MM]`, `day=[DD]`**: Este é o **particionamento por data**. É a prática mais importante para otimizar consultas baseadas em tempo. Ferramentas de Big Data podem ler apenas os dados de um dia ou mês específico, ignorando todo o resto, o que resulta em um ganho massivo de performance.\n",
    "* **`storeId=[id_da_loja]_data.json`**: O nome do arquivo final contém o ID da loja para fácil identificação e depuração.\n",
    "\n",
    "### 2.2. Implicações de uma mudança no schema (ex: `taxes` para `taxation`)\n",
    "\n",
    "Uma mudança no schema da API de origem, como a renomeação do campo `taxes` para `taxation`, tem implicações diferentes para cada camada da arquitetura:\n",
    "\n",
    "* **Impacto na Camada Bronze (Ingestão - Desafio 2):** O impacto é **nulo**. O script de ingestão é agnóstico ao schema. Sua única responsabilidade é pegar a resposta JSON da API e salvá-la. O novo arquivo será salvo com o campo `taxation` sem nenhum erro, garantindo que o dado bruto seja preservado. O pipeline de ingestão continua robusto.\n",
    "\n",
    "* **Impacto nas Camadas Silver e Gold (Transformação - Desafio 1):** O impacto é **total e crítico**. Os scripts de transformação que leem o JSON (`transform_silver.py`) e carregam os dados no SQL Server (`load_data.py`) iriam **quebrar**. Eles estão programados para procurar a chave `taxes`. Ao não encontrá-la, uma exceção seria lançada e o pipeline falharia a partir desse ponto.\n",
    "\n",
    "**Como uma solução de produção lidaria com isso:**\n",
    "\n",
    "1.  **Validação de Schema:** O pipeline de transformação deve iniciar com uma etapa de validação (usando bibliotecas como `Pydantic` ou `jsonschema`) que compara a estrutura do JSON recebido com um schema esperado.\n",
    "2.  **Código Adaptável e Versionado:** A lógica de transformação precisa ser adaptada para lidar com múltiplas versões do schema. Uma abordagem simples seria: `tax_data = check.get('taxes') or check.get('taxation')`. Em casos mais complexos, o código teria diferentes \"ramos\" para processar a v1 e a v2 do schema.\n",
    "3.  **Monitoramento e Alertas:** A falha na validação do schema deve acionar um alerta imediato para a equipe de engenharia de dados, para que possam analisar a mudança e adaptar o pipeline de transformação de forma controlada.\n",
    "\n",
    "#### 3. Processo de ETL: Pipeline Modular e Idempotente\n",
    "\n",
    "O código que move os dados entre as camadas foi escrito em Python de forma modular e com lógica de produção.\n",
    "\n",
    "* O pipeline foi dividido em scripts modulares (`ingestao_api.py`, `transform_silver.py`, `load_data.py`) e a carga final na camada Gold implementa uma lógica **Incremental do tipo \"Upsert\"**.\n",
    "\n",
    "* **Justificativa**\n",
    "    * **Manutenibilidade:** Dividir o código em módulos, cada um com uma responsabilidade única (ingestão, transformação, carga), torna o projeto muito mais fácil de entender, depurar e dar manutenção.\n",
    "    * **Lógica de Produção (Upsert):** A abordagem de \"apagar e inserir\" (`_delete_existing_order`) para registros que já existem na camada Gold garante que o Data Warehouse sempre reflita o estado mais recente e correto dos dados, lidando com correções e atualizações na origem. Isso evita a perda de dados históricos (pois não apagamos a tabela inteira) e garante a idempotência do pipeline de carga.\n",
    "    * **Transações Atômicas:** Cada pedido é processado dentro de uma transação de banco de dados. Isso garante que a carga de um pedido seja uma operação \"tudo ou nada\": ou o pedido e todos os seus detalhes são salvos com sucesso, ou, em caso de erro, toda a operação é desfeita (`rollback`), mantendo o banco de dados em um estado consistente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
